{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e2042a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "import functools\n",
    "import geopandas as gpd\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from typing import Callable, Dict, List\n",
    "\n",
    "\n",
    "us_state_to_abbrev = {\n",
    "    \"Alabama\": \"AL\",\n",
    "    \"Alaska\": \"AK\",\n",
    "    \"Arizona\": \"AZ\",\n",
    "    \"Arkansas\": \"AR\",\n",
    "    \"California\": \"CA\",\n",
    "    \"Colorado\": \"CO\",\n",
    "    \"Connecticut\": \"CT\",\n",
    "    \"Delaware\": \"DE\",\n",
    "    \"Florida\": \"FL\",\n",
    "    \"Georgia\": \"GA\",\n",
    "    \"Hawaii\": \"HI\",\n",
    "    \"Idaho\": \"ID\",\n",
    "    \"Illinois\": \"IL\",\n",
    "    \"Indiana\": \"IN\",\n",
    "    \"Iowa\": \"IA\",\n",
    "    \"Kansas\": \"KS\",\n",
    "    \"Kentucky\": \"KY\",\n",
    "    \"Louisiana\": \"LA\",\n",
    "    \"Maine\": \"ME\",\n",
    "    \"Maryland\": \"MD\",\n",
    "    \"Massachusetts\": \"MA\",\n",
    "    \"Michigan\": \"MI\",\n",
    "    \"Minnesota\": \"MN\",\n",
    "    \"Mississippi\": \"MS\",\n",
    "    \"Missouri\": \"MO\",\n",
    "    \"Montana\": \"MT\",\n",
    "    \"Nebraska\": \"NE\",\n",
    "    \"Nevada\": \"NV\",\n",
    "    \"New Hampshire\": \"NH\",\n",
    "    \"New Jersey\": \"NJ\",\n",
    "    \"New Mexico\": \"NM\",\n",
    "    \"New York\": \"NY\",\n",
    "    \"North Carolina\": \"NC\",\n",
    "    \"North Dakota\": \"ND\",\n",
    "    \"Ohio\": \"OH\",\n",
    "    \"Oklahoma\": \"OK\",\n",
    "    \"Oregon\": \"OR\",\n",
    "    \"Pennsylvania\": \"PA\",\n",
    "    \"Rhode Island\": \"RI\",\n",
    "    \"South Carolina\": \"SC\",\n",
    "    \"South Dakota\": \"SD\",\n",
    "    \"Tennessee\": \"TN\",\n",
    "    \"Texas\": \"TX\",\n",
    "    \"Utah\": \"UT\",\n",
    "    \"Vermont\": \"VT\",\n",
    "    \"Virginia\": \"VA\",\n",
    "    \"Washington\": \"WA\",\n",
    "    \"West Virginia\": \"WV\",\n",
    "    \"Wisconsin\": \"WI\",\n",
    "    \"Wyoming\": \"WY\",\n",
    "    \"District of Columbia\": \"DC\",\n",
    "    \"American Samoa\": \"AS\",\n",
    "    \"Guam\": \"GU\",\n",
    "    \"Northern Mariana Islands\": \"MP\",\n",
    "    \"Puerto Rico\": \"PR\",\n",
    "    \"United States Minor Outlying Islands\": \"UM\",\n",
    "    \"U.S. Virgin Islands\": \"VI\",\n",
    "}\n",
    "\n",
    "us_abbrev_to_state = {v: k for k, v in us_state_to_abbrev.items()}\n",
    "\n",
    "\n",
    "def load_shapes():\n",
    "    # Load CoC shapefile\n",
    "    coc_shapefile_path = 'data/CoC_GIS_National_Boundary_2022/CoC_GIS_National_Boundary.gdb'\n",
    "    coc_gdf = gpd.read_file(coc_shapefile_path)\n",
    "    # Load county shapefile\n",
    "    county_shapefile_path = 'data/tl_2022_us_county/tl_2022_us_county.shp'\n",
    "    county_gdf = gpd.read_file(county_shapefile_path)\n",
    "    # Make sure both GeoDataFrames have the same CRS (Coordinate Reference System)\n",
    "    county_gdf = county_gdf.to_crs(coc_gdf.crs)\n",
    "    # Perform spatial join between CoCs and counties\n",
    "    coc_county_gdf = gpd.sjoin(county_gdf, coc_gdf, how='inner', op='intersects', lsuffix='cnty', rsuffix='coc')\n",
    "    coc_county_gdf.columns = coc_county_gdf.columns.str.lower()\n",
    "    coc_county_gdf['county_name'] = coc_county_gdf['name']\n",
    "    coc_county_gdf['coc_number'] = coc_county_gdf['cocnum']\n",
    "    coc_county_gdf['coc_name'] = coc_county_gdf['cocname']\n",
    "    coc_county_gdf = coc_county_gdf.drop(columns=['cocnum', 'cocname', 'name'])\n",
    "    return coc_county_gdf[['coc_name', 'coc_number', 'county_name', 'state_name']]\n",
    "\n",
    "\n",
    "def load_hud_coc_data(filename: str, year: int) -> pd.DataFrame:\n",
    "    df = pd.read_excel(filename, sheet_name=str(year))\n",
    "    df.columns = [\n",
    "        col.lower()\n",
    "            .replace(',', '')\n",
    "            .replace(' - ', '_')\n",
    "            .replace(' ', '_')\n",
    "            .replace('/', '_')\n",
    "            .replace('-', '_to_')\n",
    "            .replace('(', '')\n",
    "            .replace(')', '')\n",
    "        for col in df.columns\n",
    "    ]\n",
    "    if year != 2022:\n",
    "        df = df.rename(columns={\n",
    "            f'overall_homeless_black_or_african_american_{year}': f'overall_homeless_black_african_american_or_african_{year}',\n",
    "            f'overall_homeless_asian_{year}': f'overall_homeless_asian_or_asian_american_{year}',\n",
    "        })\n",
    "    year_cols = [\n",
    "        'overall_homeless',\n",
    "        'sheltered_total_homeless',\n",
    "        'unsheltered_homeless',\n",
    "        'overall_homeless_male',\n",
    "        'overall_homeless_female',\n",
    "        'overall_homeless_white',\n",
    "        'overall_homeless_hispanic_latino',\n",
    "        'overall_homeless_black_african_american_or_african',\n",
    "        'overall_homeless_asian_or_asian_american',\n",
    "    ]\n",
    "    df = df[['coc_number', 'coc_name', *[f'{col}_{year}' for col in year_cols]]]\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_hud_affordable_units_data(filename: str, by: str, year: int) -> pd.DataFrame:\n",
    "    df = pd.read_excel(filename, header=0, usecols='A:L')\n",
    "    df.columns = [col.lower() for col in df.columns]\n",
    "    df = df[df['program_label'] == 'Summary of All HUD Programs']\n",
    "    if 'quarter' in df.columns:\n",
    "        assert all(df[df['quarter'].dt.year[0] == year])\n",
    "    cols_to_return = [\n",
    "        'state_name',\n",
    "        f'affordable_units_total_{year}', f'affordable_units_occupied_{year}', f'affordable_units_vacant_{year}',\n",
    "    ]\n",
    "    df['state_name'] = df['states'].str[3:]\n",
    "    df[f'affordable_units_occupied_{year}'] = (df['total_units'] * df['pct_occupied'] / 100).astype(np.int32)\n",
    "    df[f'affordable_units_vacant_{year}'] = (df['total_units'] - df[f'affordable_units_occupied_{year}']).astype(np.int32)\n",
    "    df = df.rename(columns={'total_units': f'affordable_units_total_{year}'})\n",
    "    if by == 'county':\n",
    "        cols_to_return.append('county_name')\n",
    "        df['county_name'] = df['name'].str.removesuffix(' County')\n",
    "        return df[cols_to_return]\n",
    "    if by == 'state':\n",
    "        df = df[df['state_name'].isin(us_state_to_abbrev.keys())]\n",
    "        df = df[cols_to_return]\n",
    "        df = df.groupby(['state_name']).sum()\n",
    "        df = df.reset_index()\n",
    "        return df[cols_to_return]\n",
    "    raise ValueError(f'Unknown value for `by`, expected `county` or `state`, got `{by}`')\n",
    "\n",
    "\n",
    "def load_usa_temperature_data(filename: str, by: str, year: int) -> pd.DataFrame:\n",
    "    df = pd.read_csv(filename)\n",
    "    df = df.rename(columns={'mean_annual_temperature': f'mean_annual_temperature_{year}'})\n",
    "    return df\n",
    "    \n",
    "    \n",
    "def load_bls_unemployment_data(filename: str, by: str) -> pd.DataFrame:\n",
    "    df = pd.read_excel(filename, index_col=0, header=4, usecols='D:I', skipfooter=3)\n",
    "    df = df[1:]\n",
    "    df = df.drop(columns=['Unnamed: 5'])\n",
    "    df = df.astype(np.int32)\n",
    "    df.columns = [col.lower().replace(' ', '_') for col in df.columns]\n",
    "    df = df.rename(columns={'force': 'labor_force'})\n",
    "    df = pd.pivot(df, columns='year', values=['labor_force', 'employed', 'unemployed'])\n",
    "    df.columns = ['_'.join(map(str, col)) for col in df.columns]\n",
    "    data_cols = df.columns\n",
    "    df = df.reset_index(names='state_and_county')\n",
    "    ddf = df['state_and_county'].str.split(', ', expand=True)\n",
    "    df = df.assign(\n",
    "        county_name=ddf[0].str.removesuffix(' County'),\n",
    "        state_name=ddf[1].map(us_abbrev_to_state),\n",
    "    )\n",
    "    df = df.reset_index()\n",
    "    df = df[['state_name', 'county_name', *data_cols]]\n",
    "    if by == 'county':\n",
    "        return df\n",
    "    if by == 'state':\n",
    "        df = df.groupby('state_name').sum()\n",
    "        df = df.drop(columns=['county_name'])\n",
    "        return df\n",
    "    raise ValueError(f'Unknown value for `by`, expected `county` or `state`, got `{by}`')\n",
    "\n",
    "\n",
    "def load_census_population_data(filename: str, by: str, year: int) -> pd.DataFrame:\n",
    "    years = [year]\n",
    "    if year < 2021:\n",
    "        df = pd.read_csv(filename)\n",
    "        df = df[df['COUNTY'] != 0]\n",
    "        col_mapping = {f'POPESTIMATE{year}': f'population_{year}' for year in years}\n",
    "        df = df.rename(columns=col_mapping)\n",
    "        df = df.assign(\n",
    "            county_name=df['CTYNAME'].str.removesuffix(' County'),\n",
    "            state_name=df['STNAME'],\n",
    "        )\n",
    "        data_cols = col_mapping.values()\n",
    "    else:\n",
    "        df = pd.read_excel(filename, index_col=0, header=3, usecols='A:E', skipfooter=5)\n",
    "        df = df.drop(columns='Unnamed: 1')\n",
    "        df.columns = [f'population_{col}' for col in df.columns]\n",
    "        data_cols = df.columns\n",
    "        df = df[1:]  # Remove row for entire United States\n",
    "        df = df.reset_index(names='state_and_county')\n",
    "        df['state_and_county'] = df['state_and_county'].str.removeprefix('.')\n",
    "        ddf = df['state_and_county'].str.split(', ', expand=True)\n",
    "        df = df.assign(\n",
    "            county_name=ddf[0].str.removesuffix(' County'),\n",
    "            state_name=ddf[1],\n",
    "        )\n",
    "    if by == 'county':\n",
    "        return df[['state_name', 'county_name', *data_cols]]\n",
    "    if by == 'state':\n",
    "        df = df.drop(columns=['county_name'])\n",
    "        df = df.groupby('state_name').sum().reset_index()\n",
    "        return df[['state_name', *data_cols]]\n",
    "    raise ValueError(f'Unknown value for `by`, expected `county` or `state`, got `{by}`')\n",
    "\n",
    "\n",
    "def load_bea_income_data(filename: str, by: str, year: int) -> pd.DataFrame:\n",
    "    years = [year]\n",
    "    if by == 'county' and year > 2020:\n",
    "        df = pd.read_excel(filename, index_col=0, header=3, usecols='A:D', skipfooter=5)\n",
    "        df.columns = [f'per_capita_income_{col}' for col in df.columns]\n",
    "        data_cols = df.columns\n",
    "        df = df[1:]  # Remove row for entire United States\n",
    "        df = df.reset_index(names='state_or_county')\n",
    "        return load_bea_data(df, data_cols)\n",
    "    if by == 'county':\n",
    "        df = pd.read_csv(filename)\n",
    "        df = df[df['Description'] == 'Per capita personal income (dollars) 2/']\n",
    "        ddf = df['GeoName'].str.split(', ', expand=True)\n",
    "        df = df.assign(\n",
    "            county_name=ddf[0],\n",
    "            state_name=ddf[1].map(us_abbrev_to_state),\n",
    "        )\n",
    "        df = df[~df['state_name'].isna()]\n",
    "        col_mapping = {str(year): f'per_capita_income_{year}' for year in years}\n",
    "        df = df.rename(columns=col_mapping)\n",
    "        for col in col_mapping.values():\n",
    "            df[col] = df[col].astype(np.float64)\n",
    "        df = df[['state_name', 'county_name', *col_mapping.values()]]\n",
    "        df = df.reset_index(drop=True)\n",
    "        return df\n",
    "    if by == 'state' and year > 2021:\n",
    "        df = pd.read_excel(filename, index_col=0, header=3, usecols='A:D', skipfooter=5)\n",
    "        df = df[df['Description'] == 'Per capita personal income (dollars) 2/']\n",
    "        ddf = df['GeoName'].str.split(', ', expand=True)\n",
    "        df = df.assign(\n",
    "            county_name=ddf[0],\n",
    "            state_name=ddf[1].map(us_abbrev_to_state),\n",
    "        )\n",
    "        df = df[df['state_name'].isin(us_state_to_abbrev.keys())]\n",
    "        col_mapping = {str(year): f'per_capita_income_{year}' for year in years}\n",
    "        df = df.rename(columns=col_mapping)\n",
    "        df = df[['state_name', 'county_name', *col_mapping.values()]]\n",
    "        return df\n",
    "    if by == 'state':\n",
    "        df = pd.read_csv(filename)\n",
    "        df = df[df['Description'] == 'Per capita personal income (dollars) 2/']\n",
    "        df = df.assign(state_name=df['GeoName'].str.removesuffix(' *'))\n",
    "        df = df[df['state_name'].isin(us_state_to_abbrev.keys())]\n",
    "        col_mapping = {str(year): f'per_capita_income_{year}' for year in years}\n",
    "        df = df.rename(columns=col_mapping)\n",
    "        df = df[['state_name', *col_mapping.values()]]\n",
    "        df = df.reset_index(drop=True)\n",
    "        return df\n",
    "    raise ValueError(f'Unknown value for `by`, expected `county` or `state`, got `{by}`')\n",
    "\n",
    "\n",
    "def load_bea_gdp_data(filename: str, by: str) -> pd.DataFrame:\n",
    "    if by != 'county':\n",
    "        raise ValueError(f'Unknown value for `by`, expected `county`, got `{by}`')\n",
    "    df = pd.read_excel(filename, index_col=0, header=3, usecols='A:E', skipfooter=4)\n",
    "    df.columns = [f'real_gdp_2012_dollars_{col}' for col in df.columns]\n",
    "    data_cols = df.columns\n",
    "    df = df * 1000  # Units are thousands of dollars. Convert to just dollars.\n",
    "    df = df[1:]  # Remove row for entire United States.\n",
    "    df = df.reset_index(names='state_or_county')\n",
    "    return load_bea_data(df, data_cols)\n",
    "\n",
    "\n",
    "def load_bea_data(df: pd.DataFrame, data_cols: List[str]) -> pd.DataFrame:\n",
    "    df = df.copy(deep=True)\n",
    "    rows = []\n",
    "    df['is_state'] = False\n",
    "    df['county_name'] = df['state_or_county']\n",
    "    is_state = False\n",
    "    for i, row in df.iterrows():\n",
    "        if is_state:\n",
    "            row.is_state = True\n",
    "            is_state = False\n",
    "        if all(row[data_cols].isna()):\n",
    "            state_name = df.loc[i+1, 'state_or_county']\n",
    "            is_state = True\n",
    "        else:\n",
    "            row['state_name'] = state_name\n",
    "        rows.append(row)\n",
    "    parsed_df = pd.DataFrame(rows)\n",
    "    parsed_df = parsed_df.dropna()\n",
    "    parsed_df = parsed_df[parsed_df['is_state'] == False]\n",
    "    parsed_df = parsed_df.drop(columns=['is_state', 'state_or_county'])\n",
    "    parsed_df = parsed_df[[\n",
    "        'state_name', 'county_name', *data_cols]]\n",
    "    return parsed_df\n",
    "\n",
    "\n",
    "def load_zillow_housing_price_data(index: str, filename: str, by: str, year: int) -> pd.DataFrame:\n",
    "    years = [year]\n",
    "    df = pd.read_csv(filename)\n",
    "    if by == 'county':\n",
    "        df['county_name'] = df['RegionName'].str.removesuffix(' County')\n",
    "        df['state_name'] = df['StateName'].map(us_abbrev_to_state)\n",
    "        index_cols = ['state_name', 'county_name']\n",
    "    elif by == 'state':\n",
    "        df['state_name'] = df['RegionName'].str.removesuffix(' County')\n",
    "        index_cols = ['state_name']\n",
    "    else:\n",
    "        raise ValueError(f'Unknown value for `by`, expected `county` or `state`, got `{by}`')\n",
    "    for year in years:\n",
    "        df[f'{index}_value_{year}'] = df[[v for v in df.columns if v.startswith(f'{year}-')]].mean(axis=1)\n",
    "    df = df[[*index_cols, *[f'{index}_value_{year}' for year in years]]]\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_opportunity_insights_social_capital_data(filename: str, by: str, year: int) -> pd.DataFrame:\n",
    "    if by != 'county':\n",
    "        raise ValueError(f'Unknown value for `by`, expected `county`, got `{by}`')\n",
    "    df = pd.read_csv(filename)\n",
    "    ddf = df['county_name'].str.split(', ', expand=True)\n",
    "    df = df.assign(\n",
    "        county_name=ddf[0],\n",
    "        state_name=ddf[1],\n",
    "    )\n",
    "    df = df[[\n",
    "        'state_name', 'county_name',\n",
    "        'num_below_p50', 'ec_county', 'ec_se_county',\n",
    "        'child_ec_county', 'child_ec_se_county', 'ec_grp_mem_county',\n",
    "        'ec_high_county', 'ec_high_se_county', 'child_high_ec_county',\n",
    "        'child_high_ec_se_county', 'ec_grp_mem_high_county',\n",
    "        'exposure_grp_mem_county', 'exposure_grp_mem_high_county',\n",
    "        'child_exposure_county', 'child_high_exposure_county',\n",
    "        'bias_grp_mem_county', 'bias_grp_mem_high_county', 'child_bias_county',\n",
    "        'child_high_bias_county', 'clustering_county', 'support_ratio_county',\n",
    "        'volunteering_rate_county', 'civic_organizations_county',\n",
    "    ]]\n",
    "    df.columns = [f'{col}_{year}' if col not in ['state_name', 'county_name'] else col for col in df.columns]\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_coc_to_county_df() -> pd.DataFrame:\n",
    "    fips_to_county_df = pd.read_csv('data/fips2county.tsv', sep='\\t')\n",
    "    fips_to_county_df = fips_to_county_df.rename(columns={\n",
    "        'StateFIPS': 'state_fips',\n",
    "        'StateName': 'state_name',\n",
    "        'CountyName': 'county_name',\n",
    "        'CountyFIPS': 'county_fips',\n",
    "    })\n",
    "    fips_to_county_df = fips_to_county_df[['state_fips', 'state_name', 'county_name', 'county_fips']]\n",
    "    fips_to_county_df\n",
    "\n",
    "    coc_mapping_df = pd.read_csv('data/county_coc_match.csv')\n",
    "    coc_mapping_df = coc_mapping_df[coc_mapping_df['rel_type'] != 4.0].groupby(['coc_name', 'coc_number', 'county_fips']).count().reset_index()\n",
    "    coc_mapping_df = coc_mapping_df[['coc_name', 'coc_number', 'county_fips']]\n",
    "    coc_mapping_df\n",
    "\n",
    "    coc_mapping_df = coc_mapping_df.merge(fips_to_county_df, on=['county_fips'], how='inner')\n",
    "    return coc_mapping_df\n",
    "\n",
    "\n",
    "def merge_dfs(dfs: List[pd.DataFrame], cols: List[str]) -> pd.DataFrame:\n",
    "    merged_df = dfs[0]\n",
    "    for i, df in enumerate(dfs[1:]):\n",
    "        merged_df = merged_df.merge(df, on=cols, how='inner')\n",
    "        print(i, len(merged_df))\n",
    "    return merged_df\n",
    "\n",
    "\n",
    "def file_in_year_range(year: int, path_regex: str) -> str:\n",
    "    files = glob.glob(f'{path_regex}_*')\n",
    "    for f in files:\n",
    "        m = re.search(f'{path_regex}_(\\d+)_to_(\\d+)\\.*', f)\n",
    "        start_year = int(m.group(1))\n",
    "        end_year = int(m.group(2))\n",
    "        if year >= start_year and year <= end_year:\n",
    "            return f\n",
    "    raise ValueError(f'Unable to find file in path for year {year}, by {by} at path {path_regex}')\n",
    "\n",
    "\n",
    "def file_path_for_unemployement_data(year: int, by: str) -> str:\n",
    "    return f'data/by_county/BLS_unemployment_by_county_{year}.xlsx'\n",
    "\n",
    "\n",
    "def file_path_for_income_data(year: int, by: str) -> str:\n",
    "    return file_in_year_range(year, f'data/by_{by}/BEA_income_by_{by}')\n",
    "\n",
    "\n",
    "def file_path_for_gdp_data(year: int, by: str) -> str:\n",
    "    return file_in_year_range(year, f'data/by_{by}/BEA_GDP_by_{by}')\n",
    "\n",
    "\n",
    "def file_path_for_social_capital_data(year: int, by: str) -> str:\n",
    "    return f'data/by_{by}/Opportunity_Insights_social_capital_by_{by}_{year}.csv'\n",
    "\n",
    "\n",
    "def file_path_for_zillow_data(index: str, year: int, by: str) -> str:\n",
    "    return file_in_year_range(year, f'data/by_{by}/Zillow_{index}_by_{by}')\n",
    "\n",
    "\n",
    "def file_path_for_affordable_units_data(year: int, by: str) -> str:\n",
    "    return f'data/by_{by}/HUD_projects_by_{by}_{year}.xlsx'\n",
    "\n",
    "\n",
    "def file_path_for_population_data(year: int, by: str) -> str:\n",
    "    return file_in_year_range(year, 'data/by_county/Census_population_by_county')\n",
    "\n",
    "def file_path_for_temperature_data(year: int, by: str) -> str:\n",
    "    return 'data/by_state/Average_annual_temperature_by_state.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7106683f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclasses.dataclass\n",
    "class Dataset:\n",
    "    load_fn: Callable\n",
    "    file_path_fn: Callable\n",
    "    sum_cols: List[str] = ()\n",
    "    mean_cols: List[str] = ()        \n",
    "\n",
    "\n",
    "ALL_DATASETS = {\n",
    "    'unemployment': Dataset(\n",
    "        load_fn=load_bls_unemployment_data,\n",
    "        file_path_fn=file_path_for_unemployement_data,\n",
    "        sum_cols=['labor_force', 'employed', 'unemployed'],\n",
    "    ),\n",
    "    'income': Dataset(\n",
    "        load_fn=load_bea_income_data,\n",
    "        file_path_fn=file_path_for_income_data,\n",
    "        mean_cols=['per_capita_income'],\n",
    "    ),\n",
    "    'gdp': Dataset(\n",
    "        load_fn=load_bea_gdp_data,\n",
    "        file_path_fn=file_path_for_gdp_data,\n",
    "        sum_cols=['real_gdp_2012_dollars'],\n",
    "    ),\n",
    "    'social_capital': Dataset(\n",
    "        load_fn=load_opportunity_insights_social_capital_data,\n",
    "        file_path_fn=file_path_for_social_capital_data,\n",
    "        mean_cols=[\n",
    "            'ec_county',\n",
    "            'child_ec_county',\n",
    "            'ec_grp_mem_county',\n",
    "            'ec_high_county',\n",
    "            'child_high_ec_county',\n",
    "            'ec_grp_mem_high_county',\n",
    "            'exposure_grp_mem_county',\n",
    "            'exposure_grp_mem_high_county',\n",
    "            'child_exposure_county',\n",
    "            'child_high_exposure_county',\n",
    "            'bias_grp_mem_county',\n",
    "            'bias_grp_mem_high_county',\n",
    "            'child_bias_county',\n",
    "            'child_high_bias_county',\n",
    "            'clustering_county',\n",
    "            'support_ratio_county',\n",
    "            'volunteering_rate_county',\n",
    "            'civic_organizations_county',\n",
    "        ],\n",
    "    ),\n",
    "    'population': Dataset(\n",
    "        load_fn=load_census_population_data,\n",
    "        file_path_fn=file_path_for_population_data,\n",
    "        sum_cols=['population'],\n",
    "    ),\n",
    "    'affordable_units': Dataset(\n",
    "        load_fn=load_hud_affordable_units_data,\n",
    "        file_path_fn=file_path_for_affordable_units_data,\n",
    "        sum_cols=['affordable_units_total', 'affordable_units_occupied', 'affordable_units_vacant'],\n",
    "    ),\n",
    "    'all_homes': Dataset(\n",
    "        load_fn=functools.partial(load_zillow_housing_price_data, 'zhvi_all_homes'),\n",
    "        file_path_fn=functools.partial(file_path_for_zillow_data, 'zhvi_all_homes'),\n",
    "        mean_cols=['zhvi_bottom_tier_value'],\n",
    "    ),\n",
    "    'bottom_tier_homes': Dataset(\n",
    "        load_fn=functools.partial(load_zillow_housing_price_data, 'zhvi_bottom_tier'),\n",
    "        file_path_fn=functools.partial(file_path_for_zillow_data, 'zhvi_bottom_tier'),\n",
    "        mean_cols=['zhvi_all_homes_value'],\n",
    "    ),\n",
    "    'rent': Dataset(\n",
    "        load_fn=functools.partial(load_zillow_housing_price_data, 'zori_rent'),\n",
    "        file_path_fn=functools.partial(file_path_for_zillow_data, 'zori_rent'),\n",
    "        mean_cols=['zori_rent_value'],\n",
    "    ),\n",
    "    'temperature': Dataset(\n",
    "        load_fn=load_usa_temperature_data,\n",
    "        file_path_fn=file_path_for_temperature_data,\n",
    "        mean_cols=['mean_annual_temperature'],\n",
    "    ),\n",
    "}\n",
    "\n",
    "\n",
    "def get_sum_cols(dataset_names: List[str], year: int) -> List[str]:\n",
    "    cols = []\n",
    "    for dataset_name in dataset_names:\n",
    "        cols += ALL_DATASETS[dataset_name].sum_cols\n",
    "    return [f'{col}_{year}' for col in cols]\n",
    "\n",
    "\n",
    "def get_mean_cols(dataset_names: List[str], year: int) -> List[str]:\n",
    "    cols = []\n",
    "    for dataset_name in dataset_names:\n",
    "        cols += ALL_DATASETS[dataset_name].mean_cols\n",
    "    return [f'{col}_{year}' for col in cols]\n",
    "\n",
    "\n",
    "def load_dataset(dataset: Dataset, year: int, by=str) -> pd.DataFrame:\n",
    "    try:\n",
    "        return dataset.load_fn(dataset.file_path_fn(year, by), by=by)\n",
    "    except TypeError:\n",
    "        return dataset.load_fn(dataset.file_path_fn(year, by), by=by, year=year)\n",
    "\n",
    "\n",
    "def load_datasets(dataset_names: List[str], year: int, by: str) -> Dict[str, pd.DataFrame]:\n",
    "    datasets = {}\n",
    "    for dataset_name in dataset_names:\n",
    "        datasets[dataset_name] = load_dataset(ALL_DATASETS[dataset_name], year, by)\n",
    "    return datasets\n",
    "\n",
    "\n",
    "def create_features_df(datasets: Dict[str, pd.DataFrame]) -> pd.DataFrame:\n",
    "    coc_to_county_df = load_coc_to_county_df()\n",
    "    one_off_datasets = ['affordable_units', 'temperature']\n",
    "    feature_merge_cols = ['state_name', 'county_name'] if by == 'county' else ['state_name']\n",
    "    dfs = [v for k, v in datasets.items() if k not in one_off_datasets]\n",
    "    features_df = merge_dfs(dfs + [coc_to_county_df], feature_merge_cols)\n",
    "    if 'affordable_units' in datasets:\n",
    "        features_df = features_df.merge(datasets['affordable_units'], how='left', on=feature_merge_cols).fillna(0)\n",
    "    if 'temperature' in datasets:\n",
    "        features_df = features_df.merge(datasets['temperature'], how='inner', on=['state_name'])\n",
    "    return features_df\n",
    "\n",
    "\n",
    "def create_model_df(features_df: pd.DataFrame, year: int, by: str) -> pd.DataFrame:\n",
    "    coc_df = load_hud_coc_data('data/HUD_2007-2022-PIT-Counts-by-CoC.xlsx', year)\n",
    "    coc_data_cols = [col for col in coc_df.columns if col not in ['coc_name', 'coc_number']]\n",
    "    model_df = features_df.merge(coc_df, on=['coc_number'], how='inner')\n",
    "    model_df = model_df.rename(columns={'coc_name_x': 'coc_name'})\n",
    "    cols = ['coc_number', 'coc_name', 'state_name']\n",
    "    if by == 'county':\n",
    "        cols.append('county_name')\n",
    "    model_df = model_df.sort_values(by=cols).reset_index(drop=True)\n",
    "    return model_df, coc_data_cols\n",
    "\n",
    "\n",
    "def create_dataset(config: DataConfig) -> pd.DataFrame:\n",
    "    year = config.year\n",
    "    by = config.by\n",
    "    dataset_names = config.dataset_names\n",
    "    datasets = load_datasets(dataset_names, year, by)\n",
    "    features_df = create_features_df(datasets)\n",
    "    model_df, coc_data_cols = create_model_df(features_df, year, by)\n",
    "    \n",
    "    coc_model_df = model_df.copy(deep=True)\n",
    "    sum_cols = get_sum_cols(dataset_names, year)\n",
    "    mean_cols = get_mean_cols(dataset_names, year)\n",
    "\n",
    "    if by == 'state':\n",
    "        grouped_by_df = coc_model_df.groupby(['state_name'])\n",
    "        coc_data_df = grouped_by_df[coc_data_cols].sum().reset_index()\n",
    "        state_data_df = grouped_by_df[sum_cols + mean_cols].mean().reset_index()\n",
    "        df = merge_dfs([coc_data_df, state_data_df], cols=['state_name'])\n",
    "        df = df.sort_values(by=['state_name'])\n",
    "    \n",
    "    elif by == 'county':\n",
    "        grouped_by_df = coc_model_df.groupby(['coc_number', 'coc_name'])\n",
    "        total_mean_cols = [f'population_weighted_{col}' for col in mean_cols]\n",
    "        coc_model_df[total_mean_cols] = coc_model_df[mean_cols].multiply(coc_model_df[f'population_{year}'], axis=0)\n",
    "        coc_data_df = grouped_by_df[coc_data_cols].mean().reset_index()\n",
    "        mean_data_df = grouped_by_df[total_mean_cols].sum().reset_index()\n",
    "        sum_data_df = grouped_by_df[sum_cols].sum().reset_index()\n",
    "        df = merge_dfs([coc_data_df, mean_data_df, sum_data_df], cols=['coc_number', 'coc_name'])\n",
    "        df[mean_cols] = df[total_mean_cols].div(df[f'population_{year}'], axis=0)\n",
    "        df = df.drop(columns=total_mean_cols)\n",
    "        df = df.sort_values(by=['coc_number'])\n",
    "        \n",
    "    df.columns = [col.removesuffix(f'_{year}') for col in df.columns]\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "75243b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclasses.dataclass\n",
    "class DataConfig:\n",
    "    year: int\n",
    "    by: str\n",
    "    dataset_names: List[str]\n",
    "    suffix: str = ''\n",
    "\n",
    "\n",
    "common_dataset_names = [\n",
    "    'unemployment',\n",
    "    'income',\n",
    "    'all_homes',\n",
    "    'bottom_tier_homes',\n",
    "    'population',\n",
    "    'affordable_units',\n",
    "    'temperature',\n",
    "]\n",
    "\n",
    "additional_dataset_names = [\n",
    "    'gdp',\n",
    "    'social_capital',\n",
    "    'rent',\n",
    "]\n",
    "\n",
    "# years = [2016, 2017, 2018, 2019, 2020, 2021]\n",
    "# years = [2017]\n",
    "# years = list(range(2011, 2022))\n",
    "years = [\n",
    "    2021,\n",
    "#     2020,  # ValueError: invalid literal for int() with base 10: 'N.A.'\n",
    "    2019,\n",
    "    2018,\n",
    "    2017,\n",
    "    2016,\n",
    "    2015,\n",
    "#     2014,  # Here and earlier: Doesn't have as granular breakdown of homeless demographics\n",
    "#     2013,\n",
    "#     2012,\n",
    "#     2011,\n",
    "]\n",
    "\n",
    "data_configs = [\n",
    "    *[DataConfig(year, 'state', common_dataset_names) for year in years],\n",
    "#     *[DataConfig(year, 'county', common_dataset_names) for year in years],\n",
    "#     DataConfig(2021, 'county', common_dataset_names + ['social_capital'], suffix='with_social_capital'),\n",
    "#     DataConfig(2021, 'county', common_dataset_names + ['rent'], suffix='with_rent'),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7677cc99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataConfig(year=2014, by='state', dataset_names=['unemployment', 'income', 'all_homes', 'bottom_tier_homes', 'population', 'affordable_units', 'temperature'], suffix='')\n",
      "0 50\n",
      "1 50\n",
      "2 50\n",
      "3 50\n",
      "4 3102\n",
      "0 50\n",
      "DataConfig(year=2013, by='state', dataset_names=['unemployment', 'income', 'all_homes', 'bottom_tier_homes', 'population', 'affordable_units', 'temperature'], suffix='')\n"
     ]
    },
    {
     "ename": "IntCastingNaNError",
     "evalue": "Cannot convert non-finite values (NA or inf) to integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 98\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(dataset, year, by)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 98\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfile_path_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43myear\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mby\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mby\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: load_hud_affordable_units_data() missing 1 required positional argument: 'year'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mIntCastingNaNError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m by \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mby\n\u001b[1;32m      5\u001b[0m suffix \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39msuffix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39msuffix \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 6\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/model/model_dataset_by_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mby\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00myear\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msuffix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)    \n",
      "Cell \u001b[0;32mIn[53], line 139\u001b[0m, in \u001b[0;36mcreate_dataset\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m    137\u001b[0m by \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mby\n\u001b[1;32m    138\u001b[0m dataset_names \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mdataset_names\n\u001b[0;32m--> 139\u001b[0m datasets \u001b[38;5;241m=\u001b[39m \u001b[43mload_datasets\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myear\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mby\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m features_df \u001b[38;5;241m=\u001b[39m create_features_df(datasets)\n\u001b[1;32m    141\u001b[0m model_df, coc_data_cols \u001b[38;5;241m=\u001b[39m create_model_df(features_df, year, by)\n",
      "Cell \u001b[0;32mIn[53], line 106\u001b[0m, in \u001b[0;36mload_datasets\u001b[0;34m(dataset_names, year, by)\u001b[0m\n\u001b[1;32m    104\u001b[0m datasets \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset_name \u001b[38;5;129;01min\u001b[39;00m dataset_names:\n\u001b[0;32m--> 106\u001b[0m     datasets[dataset_name] \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mALL_DATASETS\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myear\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mby\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m datasets\n",
      "Cell \u001b[0;32mIn[53], line 100\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(dataset, year, by)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dataset\u001b[38;5;241m.\u001b[39mload_fn(dataset\u001b[38;5;241m.\u001b[39mfile_path_fn(year, by), by\u001b[38;5;241m=\u001b[39mby)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m--> 100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfile_path_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43myear\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mby\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mby\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myear\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43myear\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[52], line 132\u001b[0m, in \u001b[0;36mload_hud_affordable_units_data\u001b[0;34m(filename, by, year)\u001b[0m\n\u001b[1;32m    127\u001b[0m cols_to_return \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstate_name\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maffordable_units_total_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00myear\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maffordable_units_occupied_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00myear\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maffordable_units_vacant_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00myear\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    130\u001b[0m ]\n\u001b[1;32m    131\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstate_name\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstates\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstr[\u001b[38;5;241m3\u001b[39m:]\n\u001b[0;32m--> 132\u001b[0m df[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maffordable_units_occupied_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00myear\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtotal_units\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpct_occupied\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m df[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maffordable_units_vacant_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00myear\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m (df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_units\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m-\u001b[39m df[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maffordable_units_occupied_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00myear\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mint32)\n\u001b[1;32m    134\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_units\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maffordable_units_total_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00myear\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m})\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/pandas/core/generic.py:6324\u001b[0m, in \u001b[0;36mNDFrame.astype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m   6317\u001b[0m     results \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   6318\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miloc[:, i]\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[1;32m   6319\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns))\n\u001b[1;32m   6320\u001b[0m     ]\n\u001b[1;32m   6322\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6323\u001b[0m     \u001b[38;5;66;03m# else, only a single dtype is given\u001b[39;00m\n\u001b[0;32m-> 6324\u001b[0m     new_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor(new_data)\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mastype\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6327\u001b[0m \u001b[38;5;66;03m# GH 33113: handle empty frame or series\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/pandas/core/internals/managers.py:451\u001b[0m, in \u001b[0;36mBaseBlockManager.astype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m using_copy_on_write():\n\u001b[1;32m    449\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 451\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mastype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m    \u001b[49m\u001b[43musing_cow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musing_copy_on_write\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/pandas/core/internals/managers.py:352\u001b[0m, in \u001b[0;36mBaseBlockManager.apply\u001b[0;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[1;32m    350\u001b[0m         applied \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39mapply(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    351\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 352\u001b[0m         applied \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    353\u001b[0m     result_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(applied, result_blocks)\n\u001b[1;32m    355\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mfrom_blocks(result_blocks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/pandas/core/internals/blocks.py:511\u001b[0m, in \u001b[0;36mBlock.astype\u001b[0;34m(self, dtype, copy, errors, using_cow)\u001b[0m\n\u001b[1;32m    491\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    492\u001b[0m \u001b[38;5;124;03mCoerce to the new dtype.\u001b[39;00m\n\u001b[1;32m    493\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;124;03mBlock\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    509\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m--> 511\u001b[0m new_values \u001b[38;5;241m=\u001b[39m \u001b[43mastype_array_safe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    513\u001b[0m new_values \u001b[38;5;241m=\u001b[39m maybe_coerce_values(new_values)\n\u001b[1;32m    515\u001b[0m refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/pandas/core/dtypes/astype.py:242\u001b[0m, in \u001b[0;36mastype_array_safe\u001b[0;34m(values, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    239\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtype\u001b[38;5;241m.\u001b[39mnumpy_dtype\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 242\u001b[0m     new_values \u001b[38;5;241m=\u001b[39m \u001b[43mastype_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;66;03m# e.g. _astype_nansafe can fail on object-dtype of strings\u001b[39;00m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;66;03m#  trying to convert to float\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/pandas/core/dtypes/astype.py:187\u001b[0m, in \u001b[0;36mastype_array\u001b[0;34m(values, dtype, copy)\u001b[0m\n\u001b[1;32m    184\u001b[0m     values \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 187\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43m_astype_nansafe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;66;03m# in pandas we don't store numpy str dtypes, so convert to object\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, np\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(values\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/pandas/core/dtypes/astype.py:105\u001b[0m, in \u001b[0;36m_astype_nansafe\u001b[0;34m(arr, dtype, copy, skipna)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mensure_string_array(\n\u001b[1;32m    101\u001b[0m         arr, skipna\u001b[38;5;241m=\u001b[39mskipna, convert_na_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    102\u001b[0m     )\u001b[38;5;241m.\u001b[39mreshape(shape)\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m np\u001b[38;5;241m.\u001b[39missubdtype(arr\u001b[38;5;241m.\u001b[39mdtype, np\u001b[38;5;241m.\u001b[39mfloating) \u001b[38;5;129;01mand\u001b[39;00m is_integer_dtype(dtype):\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_astype_float_to_int_nansafe\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_object_dtype(arr\u001b[38;5;241m.\u001b[39mdtype):\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;66;03m# if we have a datetime/timedelta array of objects\u001b[39;00m\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;66;03m# then coerce to datetime64[ns] and use DatetimeArray.astype\u001b[39;00m\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_datetime64_dtype(dtype):\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/pandas/core/dtypes/astype.py:150\u001b[0m, in \u001b[0;36m_astype_float_to_int_nansafe\u001b[0;34m(values, dtype, copy)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;124;03mastype with a check preventing converting NaN to an meaningless integer value.\u001b[39;00m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misfinite(values)\u001b[38;5;241m.\u001b[39mall():\n\u001b[0;32m--> 150\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m IntCastingNaNError(\n\u001b[1;32m    151\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot convert non-finite values (NA or inf) to integer\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    152\u001b[0m     )\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;66;03m# GH#45151\u001b[39;00m\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (values \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mall():\n",
      "\u001b[0;31mIntCastingNaNError\u001b[0m: Cannot convert non-finite values (NA or inf) to integer"
     ]
    }
   ],
   "source": [
    "for config in data_configs:\n",
    "    print(config)\n",
    "    year = config.year\n",
    "    by = config.by\n",
    "    suffix = f'_{config.suffix}' if config.suffix else ''\n",
    "    df = create_dataset(config)\n",
    "    df.to_csv(f'data/model/model_dataset_by_{by}_{year}{suffix}.csv', index=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "789ae43a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state_name</th>\n",
       "      <th>affordable_units_total_2014</th>\n",
       "      <th>affordable_units_occupied_2014</th>\n",
       "      <th>affordable_units_vacant_2014</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>93537</td>\n",
       "      <td>84183</td>\n",
       "      <td>9354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alaska</td>\n",
       "      <td>7728</td>\n",
       "      <td>6877</td>\n",
       "      <td>851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arizona</td>\n",
       "      <td>40786</td>\n",
       "      <td>37523</td>\n",
       "      <td>3263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arkansas</td>\n",
       "      <td>50996</td>\n",
       "      <td>45896</td>\n",
       "      <td>5100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>California</td>\n",
       "      <td>481569</td>\n",
       "      <td>447859</td>\n",
       "      <td>33710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Colorado</td>\n",
       "      <td>60751</td>\n",
       "      <td>56498</td>\n",
       "      <td>4253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Connecticut</td>\n",
       "      <td>81875</td>\n",
       "      <td>74506</td>\n",
       "      <td>7369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Delaware</td>\n",
       "      <td>12919</td>\n",
       "      <td>11627</td>\n",
       "      <td>1292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>District of Columbia</td>\n",
       "      <td>34114</td>\n",
       "      <td>28314</td>\n",
       "      <td>5800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Florida</td>\n",
       "      <td>195423</td>\n",
       "      <td>181743</td>\n",
       "      <td>13680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Georgia</td>\n",
       "      <td>136045</td>\n",
       "      <td>119719</td>\n",
       "      <td>16326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Guam</td>\n",
       "      <td>3350</td>\n",
       "      <td>3048</td>\n",
       "      <td>302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Hawaii</td>\n",
       "      <td>22916</td>\n",
       "      <td>17874</td>\n",
       "      <td>5042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Idaho</td>\n",
       "      <td>12359</td>\n",
       "      <td>11370</td>\n",
       "      <td>989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Illinois</td>\n",
       "      <td>229005</td>\n",
       "      <td>199234</td>\n",
       "      <td>29771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Indiana</td>\n",
       "      <td>89545</td>\n",
       "      <td>80590</td>\n",
       "      <td>8955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Iowa</td>\n",
       "      <td>40093</td>\n",
       "      <td>36083</td>\n",
       "      <td>4010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Kansas</td>\n",
       "      <td>34995</td>\n",
       "      <td>30795</td>\n",
       "      <td>4200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Kentucky</td>\n",
       "      <td>84085</td>\n",
       "      <td>74835</td>\n",
       "      <td>9250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Louisiana</td>\n",
       "      <td>96268</td>\n",
       "      <td>81827</td>\n",
       "      <td>14441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Maine</td>\n",
       "      <td>26473</td>\n",
       "      <td>24619</td>\n",
       "      <td>1854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Maryland</td>\n",
       "      <td>100267</td>\n",
       "      <td>87232</td>\n",
       "      <td>13035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>192002</td>\n",
       "      <td>180481</td>\n",
       "      <td>11521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Michigan</td>\n",
       "      <td>147036</td>\n",
       "      <td>133802</td>\n",
       "      <td>13234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Minnesota</td>\n",
       "      <td>89145</td>\n",
       "      <td>82904</td>\n",
       "      <td>6241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Mississippi</td>\n",
       "      <td>54326</td>\n",
       "      <td>48893</td>\n",
       "      <td>5433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Missouri</td>\n",
       "      <td>90973</td>\n",
       "      <td>81875</td>\n",
       "      <td>9098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Montana</td>\n",
       "      <td>13905</td>\n",
       "      <td>11819</td>\n",
       "      <td>2086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Nebraska</td>\n",
       "      <td>27692</td>\n",
       "      <td>24645</td>\n",
       "      <td>3047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Nevada</td>\n",
       "      <td>22479</td>\n",
       "      <td>21355</td>\n",
       "      <td>1124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>New Hampshire</td>\n",
       "      <td>21260</td>\n",
       "      <td>20409</td>\n",
       "      <td>851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>New Jersey</td>\n",
       "      <td>169511</td>\n",
       "      <td>155950</td>\n",
       "      <td>13561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>New Mexico</td>\n",
       "      <td>25618</td>\n",
       "      <td>21262</td>\n",
       "      <td>4356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>New York</td>\n",
       "      <td>601834</td>\n",
       "      <td>559705</td>\n",
       "      <td>42129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>North Carolina</td>\n",
       "      <td>125790</td>\n",
       "      <td>115726</td>\n",
       "      <td>10064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>North Dakota</td>\n",
       "      <td>13327</td>\n",
       "      <td>10661</td>\n",
       "      <td>2666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Northern Mariana Islands</td>\n",
       "      <td>489</td>\n",
       "      <td>405</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Ohio</td>\n",
       "      <td>224101</td>\n",
       "      <td>208413</td>\n",
       "      <td>15688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Oklahoma</td>\n",
       "      <td>53111</td>\n",
       "      <td>49393</td>\n",
       "      <td>3718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Oregon</td>\n",
       "      <td>52816</td>\n",
       "      <td>48590</td>\n",
       "      <td>4226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>220799</td>\n",
       "      <td>198719</td>\n",
       "      <td>22080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>107750</td>\n",
       "      <td>100207</td>\n",
       "      <td>7543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Rhode Island</td>\n",
       "      <td>37631</td>\n",
       "      <td>34996</td>\n",
       "      <td>2635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>South Carolina</td>\n",
       "      <td>62759</td>\n",
       "      <td>58993</td>\n",
       "      <td>3766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>South Dakota</td>\n",
       "      <td>13825</td>\n",
       "      <td>11889</td>\n",
       "      <td>1936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Tennessee</td>\n",
       "      <td>105442</td>\n",
       "      <td>100169</td>\n",
       "      <td>5273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Texas</td>\n",
       "      <td>278808</td>\n",
       "      <td>250927</td>\n",
       "      <td>27881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>U.S. Virgin Islands</td>\n",
       "      <td>6486</td>\n",
       "      <td>5253</td>\n",
       "      <td>1233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Utah</td>\n",
       "      <td>18584</td>\n",
       "      <td>18026</td>\n",
       "      <td>558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Vermont</td>\n",
       "      <td>12386</td>\n",
       "      <td>11023</td>\n",
       "      <td>1363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Virginia</td>\n",
       "      <td>104129</td>\n",
       "      <td>93716</td>\n",
       "      <td>10413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Washington</td>\n",
       "      <td>86989</td>\n",
       "      <td>80029</td>\n",
       "      <td>6960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>West Virginia</td>\n",
       "      <td>34039</td>\n",
       "      <td>30975</td>\n",
       "      <td>3064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>75824</td>\n",
       "      <td>69758</td>\n",
       "      <td>6066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>5663</td>\n",
       "      <td>5096</td>\n",
       "      <td>567</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  state_name  affordable_units_total_2014   \n",
       "0                    Alabama                        93537  \\\n",
       "1                     Alaska                         7728   \n",
       "2                    Arizona                        40786   \n",
       "3                   Arkansas                        50996   \n",
       "4                 California                       481569   \n",
       "5                   Colorado                        60751   \n",
       "6                Connecticut                        81875   \n",
       "7                   Delaware                        12919   \n",
       "8       District of Columbia                        34114   \n",
       "9                    Florida                       195423   \n",
       "10                   Georgia                       136045   \n",
       "11                      Guam                         3350   \n",
       "12                    Hawaii                        22916   \n",
       "13                     Idaho                        12359   \n",
       "14                  Illinois                       229005   \n",
       "15                   Indiana                        89545   \n",
       "16                      Iowa                        40093   \n",
       "17                    Kansas                        34995   \n",
       "18                  Kentucky                        84085   \n",
       "19                 Louisiana                        96268   \n",
       "20                     Maine                        26473   \n",
       "21                  Maryland                       100267   \n",
       "22             Massachusetts                       192002   \n",
       "23                  Michigan                       147036   \n",
       "24                 Minnesota                        89145   \n",
       "25               Mississippi                        54326   \n",
       "26                  Missouri                        90973   \n",
       "27                   Montana                        13905   \n",
       "28                  Nebraska                        27692   \n",
       "29                    Nevada                        22479   \n",
       "30             New Hampshire                        21260   \n",
       "31                New Jersey                       169511   \n",
       "32                New Mexico                        25618   \n",
       "33                  New York                       601834   \n",
       "34            North Carolina                       125790   \n",
       "35              North Dakota                        13327   \n",
       "36  Northern Mariana Islands                          489   \n",
       "37                      Ohio                       224101   \n",
       "38                  Oklahoma                        53111   \n",
       "39                    Oregon                        52816   \n",
       "40              Pennsylvania                       220799   \n",
       "41               Puerto Rico                       107750   \n",
       "42              Rhode Island                        37631   \n",
       "43            South Carolina                        62759   \n",
       "44              South Dakota                        13825   \n",
       "45                 Tennessee                       105442   \n",
       "46                     Texas                       278808   \n",
       "47       U.S. Virgin Islands                         6486   \n",
       "48                      Utah                        18584   \n",
       "49                   Vermont                        12386   \n",
       "50                  Virginia                       104129   \n",
       "51                Washington                        86989   \n",
       "52             West Virginia                        34039   \n",
       "53                 Wisconsin                        75824   \n",
       "54                   Wyoming                         5663   \n",
       "\n",
       "    affordable_units_occupied_2014  affordable_units_vacant_2014  \n",
       "0                            84183                          9354  \n",
       "1                             6877                           851  \n",
       "2                            37523                          3263  \n",
       "3                            45896                          5100  \n",
       "4                           447859                         33710  \n",
       "5                            56498                          4253  \n",
       "6                            74506                          7369  \n",
       "7                            11627                          1292  \n",
       "8                            28314                          5800  \n",
       "9                           181743                         13680  \n",
       "10                          119719                         16326  \n",
       "11                            3048                           302  \n",
       "12                           17874                          5042  \n",
       "13                           11370                           989  \n",
       "14                          199234                         29771  \n",
       "15                           80590                          8955  \n",
       "16                           36083                          4010  \n",
       "17                           30795                          4200  \n",
       "18                           74835                          9250  \n",
       "19                           81827                         14441  \n",
       "20                           24619                          1854  \n",
       "21                           87232                         13035  \n",
       "22                          180481                         11521  \n",
       "23                          133802                         13234  \n",
       "24                           82904                          6241  \n",
       "25                           48893                          5433  \n",
       "26                           81875                          9098  \n",
       "27                           11819                          2086  \n",
       "28                           24645                          3047  \n",
       "29                           21355                          1124  \n",
       "30                           20409                           851  \n",
       "31                          155950                         13561  \n",
       "32                           21262                          4356  \n",
       "33                          559705                         42129  \n",
       "34                          115726                         10064  \n",
       "35                           10661                          2666  \n",
       "36                             405                            84  \n",
       "37                          208413                         15688  \n",
       "38                           49393                          3718  \n",
       "39                           48590                          4226  \n",
       "40                          198719                         22080  \n",
       "41                          100207                          7543  \n",
       "42                           34996                          2635  \n",
       "43                           58993                          3766  \n",
       "44                           11889                          1936  \n",
       "45                          100169                          5273  \n",
       "46                          250927                         27881  \n",
       "47                            5253                          1233  \n",
       "48                           18026                           558  \n",
       "49                           11023                          1363  \n",
       "50                           93716                         10413  \n",
       "51                           80029                          6960  \n",
       "52                           30975                          3064  \n",
       "53                           69758                          6066  \n",
       "54                            5096                           567  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_hud_affordable_units_data(file_path_for_affordable_units_data(2014, 'state'), by='state', year=2014)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb95348",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
