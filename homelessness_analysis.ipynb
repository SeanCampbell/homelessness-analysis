{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e2042a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "import functools\n",
    "import geopandas as gpd\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from typing import Callable, Dict, List\n",
    "\n",
    "\n",
    "us_state_to_abbrev = {\n",
    "    \"Alabama\": \"AL\",\n",
    "    \"Alaska\": \"AK\",\n",
    "    \"Arizona\": \"AZ\",\n",
    "    \"Arkansas\": \"AR\",\n",
    "    \"California\": \"CA\",\n",
    "    \"Colorado\": \"CO\",\n",
    "    \"Connecticut\": \"CT\",\n",
    "    \"Delaware\": \"DE\",\n",
    "    \"Florida\": \"FL\",\n",
    "    \"Georgia\": \"GA\",\n",
    "    \"Hawaii\": \"HI\",\n",
    "    \"Idaho\": \"ID\",\n",
    "    \"Illinois\": \"IL\",\n",
    "    \"Indiana\": \"IN\",\n",
    "    \"Iowa\": \"IA\",\n",
    "    \"Kansas\": \"KS\",\n",
    "    \"Kentucky\": \"KY\",\n",
    "    \"Louisiana\": \"LA\",\n",
    "    \"Maine\": \"ME\",\n",
    "    \"Maryland\": \"MD\",\n",
    "    \"Massachusetts\": \"MA\",\n",
    "    \"Michigan\": \"MI\",\n",
    "    \"Minnesota\": \"MN\",\n",
    "    \"Mississippi\": \"MS\",\n",
    "    \"Missouri\": \"MO\",\n",
    "    \"Montana\": \"MT\",\n",
    "    \"Nebraska\": \"NE\",\n",
    "    \"Nevada\": \"NV\",\n",
    "    \"New Hampshire\": \"NH\",\n",
    "    \"New Jersey\": \"NJ\",\n",
    "    \"New Mexico\": \"NM\",\n",
    "    \"New York\": \"NY\",\n",
    "    \"North Carolina\": \"NC\",\n",
    "    \"North Dakota\": \"ND\",\n",
    "    \"Ohio\": \"OH\",\n",
    "    \"Oklahoma\": \"OK\",\n",
    "    \"Oregon\": \"OR\",\n",
    "    \"Pennsylvania\": \"PA\",\n",
    "    \"Rhode Island\": \"RI\",\n",
    "    \"South Carolina\": \"SC\",\n",
    "    \"South Dakota\": \"SD\",\n",
    "    \"Tennessee\": \"TN\",\n",
    "    \"Texas\": \"TX\",\n",
    "    \"Utah\": \"UT\",\n",
    "    \"Vermont\": \"VT\",\n",
    "    \"Virginia\": \"VA\",\n",
    "    \"Washington\": \"WA\",\n",
    "    \"West Virginia\": \"WV\",\n",
    "    \"Wisconsin\": \"WI\",\n",
    "    \"Wyoming\": \"WY\",\n",
    "    \"District of Columbia\": \"DC\",\n",
    "    \"American Samoa\": \"AS\",\n",
    "    \"Guam\": \"GU\",\n",
    "    \"Northern Mariana Islands\": \"MP\",\n",
    "    \"Puerto Rico\": \"PR\",\n",
    "    \"United States Minor Outlying Islands\": \"UM\",\n",
    "    \"U.S. Virgin Islands\": \"VI\",\n",
    "}\n",
    "\n",
    "us_abbrev_to_state = {v: k for k, v in us_state_to_abbrev.items()}\n",
    "\n",
    "\n",
    "def load_shapes():\n",
    "    # Load CoC shapefile\n",
    "    coc_shapefile_path = 'data/CoC_GIS_National_Boundary_2022/CoC_GIS_National_Boundary.gdb'\n",
    "    coc_gdf = gpd.read_file(coc_shapefile_path)\n",
    "    # Load county shapefile\n",
    "    county_shapefile_path = 'data/tl_2022_us_county/tl_2022_us_county.shp'\n",
    "    county_gdf = gpd.read_file(county_shapefile_path)\n",
    "    # Make sure both GeoDataFrames have the same CRS (Coordinate Reference System)\n",
    "    county_gdf = county_gdf.to_crs(coc_gdf.crs)\n",
    "    # Perform spatial join between CoCs and counties\n",
    "    coc_county_gdf = gpd.sjoin(county_gdf, coc_gdf, how='inner', op='intersects', lsuffix='cnty', rsuffix='coc')\n",
    "    coc_county_gdf.columns = coc_county_gdf.columns.str.lower()\n",
    "    coc_county_gdf['county_name'] = coc_county_gdf['name']\n",
    "    coc_county_gdf['coc_number'] = coc_county_gdf['cocnum']\n",
    "    coc_county_gdf['coc_name'] = coc_county_gdf['cocname']\n",
    "    coc_county_gdf = coc_county_gdf.drop(columns=['cocnum', 'cocname', 'name'])\n",
    "    return coc_county_gdf[['coc_name', 'coc_number', 'county_name', 'state_name']]\n",
    "\n",
    "\n",
    "def load_hud_coc_data(filename: str, year: int) -> pd.DataFrame:\n",
    "    df = pd.read_excel(filename, sheet_name=str(year))\n",
    "    df.columns = [\n",
    "        col.lower()\n",
    "            .replace(',', '')\n",
    "            .replace(' - ', '_')\n",
    "            .replace(' ', '_')\n",
    "            .replace('/', '_')\n",
    "            .replace('-', '_to_')\n",
    "            .replace('(', '')\n",
    "            .replace(')', '')\n",
    "        for col in df.columns\n",
    "    ]\n",
    "    if year != 2022:\n",
    "        df = df.rename(columns={\n",
    "            f'overall_homeless_black_or_african_american_{year}': f'overall_homeless_black_african_american_or_african_{year}',\n",
    "            f'overall_homeless_asian_{year}': f'overall_homeless_asian_or_asian_american_{year}',\n",
    "        })\n",
    "    year_cols = [\n",
    "        'overall_homeless',\n",
    "        'sheltered_total_homeless',\n",
    "        'unsheltered_homeless',\n",
    "        'overall_homeless_male',\n",
    "        'overall_homeless_female',\n",
    "        'overall_homeless_white',\n",
    "        'overall_homeless_hispanic_latino',\n",
    "        'overall_homeless_black_african_american_or_african',\n",
    "        'overall_homeless_asian_or_asian_american',\n",
    "    ]\n",
    "    df = df[['coc_number', 'coc_name', *[f'{col}_{year}' for col in year_cols]]]\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_hud_affordable_units_data(filename: str, by: str, year: int) -> pd.DataFrame:\n",
    "    df = pd.read_excel(filename, header=0, usecols='A:L')\n",
    "    df.columns = [col.lower() for col in df.columns]\n",
    "    df = df[df['program_label'] == 'Summary of All HUD Programs']\n",
    "    if 'quarter' in df.columns:\n",
    "        assert all(df['quarter'].dt.year == year)\n",
    "    cols_to_return = [\n",
    "        'state_name',\n",
    "        f'affordable_units_total_{year}', f'affordable_units_occupied_{year}', f'affordable_units_vacant_{year}',\n",
    "    ]\n",
    "    df['state_name'] = df['states'].str[3:]\n",
    "    df[f'affordable_units_occupied_{year}'] = (df['total_units'] * df['pct_occupied'] / 100).astype(np.int32)\n",
    "    df[f'affordable_units_vacant_{year}'] = (df['total_units'] - df[f'affordable_units_occupied_{year}']).astype(np.int32)\n",
    "    df = df.rename(columns={'total_units': f'affordable_units_total_{year}'})\n",
    "    if by == 'county':\n",
    "        cols_to_return.append('county_name')\n",
    "        df['county_name'] = df['name'].str.removesuffix(' County')\n",
    "        return df[cols_to_return]\n",
    "    if by == 'state':\n",
    "        df = df[df['state_name'].isin(us_state_to_abbrev.keys())]\n",
    "        df = df[cols_to_return]\n",
    "        df = df.groupby(['state_name']).sum()\n",
    "        df = df.reset_index()\n",
    "        return df[cols_to_return]\n",
    "    raise ValueError(f'Unknown value for `by`, expected `county` or `state`, got `{by}`')\n",
    "\n",
    "\n",
    "def load_usa_temperature_data(filename: str, by: str, year: int) -> pd.DataFrame:\n",
    "    df = pd.read_csv(filename)\n",
    "    df = df.rename(columns={'mean_annual_temperature': f'mean_annual_temperature_{year}'})\n",
    "    return df\n",
    "    \n",
    "    \n",
    "def load_bls_unemployment_data(filename: str, by: str) -> pd.DataFrame:\n",
    "    df = pd.read_excel(filename, index_col=0, header=4, usecols='D:I', skipfooter=3, keep_default_na=True, na_values='N.A.')\n",
    "    df = df[1:]\n",
    "    df = df.drop(columns=['Unnamed: 5'])\n",
    "    df = df.dropna()\n",
    "    df = df.astype(np.int32)\n",
    "    df.columns = [col.lower().replace(' ', '_') for col in df.columns]\n",
    "    df = df.rename(columns={'force': 'labor_force'})\n",
    "    df = pd.pivot(df, columns='year', values=['labor_force', 'employed', 'unemployed'])\n",
    "    df.columns = ['_'.join(map(str, col)) for col in df.columns]\n",
    "    data_cols = df.columns\n",
    "    df = df.reset_index(names='state_and_county')\n",
    "    ddf = df['state_and_county'].str.split(', ', expand=True)\n",
    "    df = df.assign(\n",
    "        county_name=ddf[0].str.removesuffix(' County'),\n",
    "        state_name=ddf[1].map(us_abbrev_to_state),\n",
    "    )\n",
    "    df = df.reset_index()\n",
    "    df = df[['state_name', 'county_name', *data_cols]]\n",
    "    if by == 'county':\n",
    "        return df\n",
    "    if by == 'state':\n",
    "        df = df.groupby('state_name').sum()\n",
    "        df = df.drop(columns=['county_name'])\n",
    "        return df\n",
    "    raise ValueError(f'Unknown value for `by`, expected `county` or `state`, got `{by}`')\n",
    "\n",
    "\n",
    "def load_census_population_data(filename: str, by: str, year: int) -> pd.DataFrame:\n",
    "    years = [year]\n",
    "    if year < 2021:\n",
    "        df = pd.read_csv(filename)\n",
    "        df = df[df['COUNTY'] != 0]\n",
    "        col_mapping = {f'POPESTIMATE{year}': f'population_{year}' for year in years}\n",
    "        df = df.rename(columns=col_mapping)\n",
    "        df = df.assign(\n",
    "            county_name=df['CTYNAME'].str.removesuffix(' County'),\n",
    "            state_name=df['STNAME'],\n",
    "        )\n",
    "        data_cols = col_mapping.values()\n",
    "    else:\n",
    "        df = pd.read_excel(filename, index_col=0, header=3, usecols='A:E', skipfooter=5)\n",
    "        df = df.drop(columns='Unnamed: 1')\n",
    "        df.columns = [f'population_{col}' for col in df.columns]\n",
    "        data_cols = df.columns\n",
    "        df = df[1:]  # Remove row for entire United States\n",
    "        df = df.reset_index(names='state_and_county')\n",
    "        df['state_and_county'] = df['state_and_county'].str.removeprefix('.')\n",
    "        ddf = df['state_and_county'].str.split(', ', expand=True)\n",
    "        df = df.assign(\n",
    "            county_name=ddf[0].str.removesuffix(' County'),\n",
    "            state_name=ddf[1],\n",
    "        )\n",
    "    if by == 'county':\n",
    "        return df[['state_name', 'county_name', *data_cols]]\n",
    "    if by == 'state':\n",
    "        df = df.drop(columns=['county_name'])\n",
    "        df = df.groupby('state_name').sum().reset_index()\n",
    "        return df[['state_name', *data_cols]]\n",
    "    raise ValueError(f'Unknown value for `by`, expected `county` or `state`, got `{by}`')\n",
    "\n",
    "\n",
    "def load_bea_income_data(filename: str, by: str, year: int) -> pd.DataFrame:\n",
    "    years = [year]\n",
    "    if by == 'county' and year > 2020:\n",
    "        df = pd.read_excel(filename, index_col=0, header=3, usecols='A:D', skipfooter=5)\n",
    "        df.columns = [f'per_capita_income_{col}' for col in df.columns]\n",
    "        data_cols = df.columns\n",
    "        df = df[1:]  # Remove row for entire United States\n",
    "        df = df.reset_index(names='state_or_county')\n",
    "        return load_bea_data(df, data_cols)\n",
    "    if by == 'county':\n",
    "        df = pd.read_csv(filename)\n",
    "        df = df[df['Description'] == 'Per capita personal income (dollars) 2/']\n",
    "        ddf = df['GeoName'].str.split(', ', expand=True)\n",
    "        df = df.assign(\n",
    "            county_name=ddf[0],\n",
    "            state_name=ddf[1].map(us_abbrev_to_state),\n",
    "        )\n",
    "        df = df[~df['state_name'].isna()]\n",
    "        col_mapping = {str(year): f'per_capita_income_{year}' for year in years}\n",
    "        df = df.rename(columns=col_mapping)\n",
    "        for col in col_mapping.values():\n",
    "            df[col] = df[col].astype(np.float64)\n",
    "        df = df[['state_name', 'county_name', *col_mapping.values()]]\n",
    "        df = df.reset_index(drop=True)\n",
    "        return df\n",
    "    if by == 'state' and year > 2021:\n",
    "        df = pd.read_excel(filename, index_col=0, header=3, usecols='A:D', skipfooter=5)\n",
    "        df = df[df['Description'] == 'Per capita personal income (dollars) 2/']\n",
    "        ddf = df['GeoName'].str.split(', ', expand=True)\n",
    "        df = df.assign(\n",
    "            county_name=ddf[0],\n",
    "            state_name=ddf[1].map(us_abbrev_to_state),\n",
    "        )\n",
    "        df = df[df['state_name'].isin(us_state_to_abbrev.keys())]\n",
    "        col_mapping = {str(year): f'per_capita_income_{year}' for year in years}\n",
    "        df = df.rename(columns=col_mapping)\n",
    "        df = df[['state_name', 'county_name', *col_mapping.values()]]\n",
    "        return df\n",
    "    if by == 'state':\n",
    "        df = pd.read_csv(filename)\n",
    "        df = df[df['Description'] == 'Per capita personal income (dollars) 2/']\n",
    "        df = df.assign(state_name=df['GeoName'].str.removesuffix(' *'))\n",
    "        df = df[df['state_name'].isin(us_state_to_abbrev.keys())]\n",
    "        col_mapping = {str(year): f'per_capita_income_{year}' for year in years}\n",
    "        df = df.rename(columns=col_mapping)\n",
    "        df = df[['state_name', *col_mapping.values()]]\n",
    "        df = df.reset_index(drop=True)\n",
    "        return df\n",
    "    raise ValueError(f'Unknown value for `by`, expected `county` or `state`, got `{by}`')\n",
    "\n",
    "\n",
    "def load_bea_gdp_data(filename: str, by: str) -> pd.DataFrame:\n",
    "    if by != 'county':\n",
    "        raise ValueError(f'Unknown value for `by`, expected `county`, got `{by}`')\n",
    "    df = pd.read_excel(filename, index_col=0, header=3, usecols='A:E', skipfooter=4)\n",
    "    df.columns = [f'real_gdp_2012_dollars_{col}' for col in df.columns]\n",
    "    data_cols = df.columns\n",
    "    df = df * 1000  # Units are thousands of dollars. Convert to just dollars.\n",
    "    df = df[1:]  # Remove row for entire United States.\n",
    "    df = df.reset_index(names='state_or_county')\n",
    "    return load_bea_data(df, data_cols)\n",
    "\n",
    "\n",
    "def load_bea_data(df: pd.DataFrame, data_cols: List[str]) -> pd.DataFrame:\n",
    "    df = df.copy(deep=True)\n",
    "    rows = []\n",
    "    df['is_state'] = False\n",
    "    df['county_name'] = df['state_or_county']\n",
    "    is_state = False\n",
    "    for i, row in df.iterrows():\n",
    "        if is_state:\n",
    "            row.is_state = True\n",
    "            is_state = False\n",
    "        if all(row[data_cols].isna()):\n",
    "            state_name = df.loc[i+1, 'state_or_county']\n",
    "            is_state = True\n",
    "        else:\n",
    "            row['state_name'] = state_name\n",
    "        rows.append(row)\n",
    "    parsed_df = pd.DataFrame(rows)\n",
    "    parsed_df = parsed_df.dropna()\n",
    "    parsed_df = parsed_df[parsed_df['is_state'] == False]\n",
    "    parsed_df = parsed_df.drop(columns=['is_state', 'state_or_county'])\n",
    "    parsed_df = parsed_df[[\n",
    "        'state_name', 'county_name', *data_cols]]\n",
    "    return parsed_df\n",
    "\n",
    "\n",
    "def load_zillow_housing_price_data(index: str, filename: str, by: str, year: int) -> pd.DataFrame:\n",
    "    years = [year]\n",
    "    df = pd.read_csv(filename)\n",
    "    if by == 'county':\n",
    "        df['county_name'] = df['RegionName'].str.removesuffix(' County')\n",
    "        df['state_name'] = df['StateName'].map(us_abbrev_to_state)\n",
    "        index_cols = ['state_name', 'county_name']\n",
    "    elif by == 'state':\n",
    "        df['state_name'] = df['RegionName'].str.removesuffix(' County')\n",
    "        index_cols = ['state_name']\n",
    "    else:\n",
    "        raise ValueError(f'Unknown value for `by`, expected `county` or `state`, got `{by}`')\n",
    "    for year in years:\n",
    "        df[f'{index}_value_{year}'] = df[[v for v in df.columns if v.startswith(f'{year}-')]].mean(axis=1)\n",
    "    df = df[[*index_cols, *[f'{index}_value_{year}' for year in years]]]\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_opportunity_insights_social_capital_data(filename: str, by: str, year: int) -> pd.DataFrame:\n",
    "    if by != 'county':\n",
    "        raise ValueError(f'Unknown value for `by`, expected `county`, got `{by}`')\n",
    "    df = pd.read_csv(filename)\n",
    "    ddf = df['county_name'].str.split(', ', expand=True)\n",
    "    df = df.assign(\n",
    "        county_name=ddf[0],\n",
    "        state_name=ddf[1],\n",
    "    )\n",
    "    df = df[[\n",
    "        'state_name', 'county_name',\n",
    "        'num_below_p50', 'ec_county', 'ec_se_county',\n",
    "        'child_ec_county', 'child_ec_se_county', 'ec_grp_mem_county',\n",
    "        'ec_high_county', 'ec_high_se_county', 'child_high_ec_county',\n",
    "        'child_high_ec_se_county', 'ec_grp_mem_high_county',\n",
    "        'exposure_grp_mem_county', 'exposure_grp_mem_high_county',\n",
    "        'child_exposure_county', 'child_high_exposure_county',\n",
    "        'bias_grp_mem_county', 'bias_grp_mem_high_county', 'child_bias_county',\n",
    "        'child_high_bias_county', 'clustering_county', 'support_ratio_county',\n",
    "        'volunteering_rate_county', 'civic_organizations_county',\n",
    "    ]]\n",
    "    df.columns = [f'{col}_{year}' if col not in ['state_name', 'county_name'] else col for col in df.columns]\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_coc_to_county_df() -> pd.DataFrame:\n",
    "    fips_to_county_df = pd.read_csv('data/fips2county.tsv', sep='\\t')\n",
    "    fips_to_county_df = fips_to_county_df.rename(columns={\n",
    "        'StateFIPS': 'state_fips',\n",
    "        'StateName': 'state_name',\n",
    "        'CountyName': 'county_name',\n",
    "        'CountyFIPS': 'county_fips',\n",
    "    })\n",
    "    fips_to_county_df = fips_to_county_df[['state_fips', 'state_name', 'county_name', 'county_fips']]\n",
    "    fips_to_county_df\n",
    "\n",
    "    coc_mapping_df = pd.read_csv('data/county_coc_match.csv')\n",
    "    coc_mapping_df = coc_mapping_df[coc_mapping_df['rel_type'] != 4.0].groupby(['coc_name', 'coc_number', 'county_fips']).count().reset_index()\n",
    "    coc_mapping_df = coc_mapping_df[['coc_name', 'coc_number', 'county_fips']]\n",
    "    coc_mapping_df\n",
    "\n",
    "    coc_mapping_df = coc_mapping_df.merge(fips_to_county_df, on=['county_fips'], how='inner')\n",
    "    return coc_mapping_df\n",
    "\n",
    "\n",
    "def merge_dfs(dfs: List[pd.DataFrame], cols: List[str]) -> pd.DataFrame:\n",
    "    merged_df = dfs[0]\n",
    "    for i, df in enumerate(dfs[1:]):\n",
    "        merged_df = merged_df.merge(df, on=cols, how='inner')\n",
    "        print(i, len(merged_df))\n",
    "    return merged_df\n",
    "\n",
    "\n",
    "def file_in_year_range(year: int, path_regex: str) -> str:\n",
    "    files = glob.glob(f'{path_regex}_*')\n",
    "    for f in files:\n",
    "        m = re.search(f'{path_regex}_(\\d+)_to_(\\d+)\\.*', f)\n",
    "        start_year = int(m.group(1))\n",
    "        end_year = int(m.group(2))\n",
    "        if year >= start_year and year <= end_year:\n",
    "            return f\n",
    "    raise ValueError(f'Unable to find file in path for year {year}, by {by} at path {path_regex}')\n",
    "\n",
    "\n",
    "def file_path_for_unemployement_data(year: int, by: str) -> str:\n",
    "    return f'data/by_county/BLS_unemployment_by_county_{year}.xlsx'\n",
    "\n",
    "\n",
    "def file_path_for_income_data(year: int, by: str) -> str:\n",
    "    return file_in_year_range(year, f'data/by_{by}/BEA_income_by_{by}')\n",
    "\n",
    "\n",
    "def file_path_for_gdp_data(year: int, by: str) -> str:\n",
    "    return file_in_year_range(year, f'data/by_{by}/BEA_GDP_by_{by}')\n",
    "\n",
    "\n",
    "def file_path_for_social_capital_data(year: int, by: str) -> str:\n",
    "    return f'data/by_{by}/Opportunity_Insights_social_capital_by_{by}_{year}.csv'\n",
    "\n",
    "\n",
    "def file_path_for_zillow_data(index: str, year: int, by: str) -> str:\n",
    "    return file_in_year_range(year, f'data/by_{by}/Zillow_{index}_by_{by}')\n",
    "\n",
    "\n",
    "def file_path_for_affordable_units_data(year: int, by: str) -> str:\n",
    "    return f'data/by_{by}/HUD_projects_by_{by}_{year}.xlsx'\n",
    "\n",
    "\n",
    "def file_path_for_population_data(year: int, by: str) -> str:\n",
    "    return file_in_year_range(year, 'data/by_county/Census_population_by_county')\n",
    "\n",
    "def file_path_for_temperature_data(year: int, by: str) -> str:\n",
    "    return 'data/by_state/Average_annual_temperature_by_state.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7106683f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclasses.dataclass\n",
    "class Dataset:\n",
    "    load_fn: Callable\n",
    "    file_path_fn: Callable\n",
    "    sum_cols: List[str] = ()\n",
    "    mean_cols: List[str] = ()        \n",
    "\n",
    "\n",
    "ALL_DATASETS = {\n",
    "    'unemployment': Dataset(\n",
    "        load_fn=load_bls_unemployment_data,\n",
    "        file_path_fn=file_path_for_unemployement_data,\n",
    "        sum_cols=['labor_force', 'employed', 'unemployed'],\n",
    "    ),\n",
    "    'income': Dataset(\n",
    "        load_fn=load_bea_income_data,\n",
    "        file_path_fn=file_path_for_income_data,\n",
    "        mean_cols=['per_capita_income'],\n",
    "    ),\n",
    "    'gdp': Dataset(\n",
    "        load_fn=load_bea_gdp_data,\n",
    "        file_path_fn=file_path_for_gdp_data,\n",
    "        sum_cols=['real_gdp_2012_dollars'],\n",
    "    ),\n",
    "    'social_capital': Dataset(\n",
    "        load_fn=load_opportunity_insights_social_capital_data,\n",
    "        file_path_fn=file_path_for_social_capital_data,\n",
    "        mean_cols=[\n",
    "            'ec_county',\n",
    "            'child_ec_county',\n",
    "            'ec_grp_mem_county',\n",
    "            'ec_high_county',\n",
    "            'child_high_ec_county',\n",
    "            'ec_grp_mem_high_county',\n",
    "            'exposure_grp_mem_county',\n",
    "            'exposure_grp_mem_high_county',\n",
    "            'child_exposure_county',\n",
    "            'child_high_exposure_county',\n",
    "            'bias_grp_mem_county',\n",
    "            'bias_grp_mem_high_county',\n",
    "            'child_bias_county',\n",
    "            'child_high_bias_county',\n",
    "            'clustering_county',\n",
    "            'support_ratio_county',\n",
    "            'volunteering_rate_county',\n",
    "            'civic_organizations_county',\n",
    "        ],\n",
    "    ),\n",
    "    'population': Dataset(\n",
    "        load_fn=load_census_population_data,\n",
    "        file_path_fn=file_path_for_population_data,\n",
    "        sum_cols=['population'],\n",
    "    ),\n",
    "    'affordable_units': Dataset(\n",
    "        load_fn=load_hud_affordable_units_data,\n",
    "        file_path_fn=file_path_for_affordable_units_data,\n",
    "        sum_cols=['affordable_units_total', 'affordable_units_occupied', 'affordable_units_vacant'],\n",
    "    ),\n",
    "    'all_homes': Dataset(\n",
    "        load_fn=functools.partial(load_zillow_housing_price_data, 'zhvi_all_homes'),\n",
    "        file_path_fn=functools.partial(file_path_for_zillow_data, 'zhvi_all_homes'),\n",
    "        mean_cols=['zhvi_bottom_tier_value'],\n",
    "    ),\n",
    "    'bottom_tier_homes': Dataset(\n",
    "        load_fn=functools.partial(load_zillow_housing_price_data, 'zhvi_bottom_tier'),\n",
    "        file_path_fn=functools.partial(file_path_for_zillow_data, 'zhvi_bottom_tier'),\n",
    "        mean_cols=['zhvi_all_homes_value'],\n",
    "    ),\n",
    "    'rent': Dataset(\n",
    "        load_fn=functools.partial(load_zillow_housing_price_data, 'zori_rent'),\n",
    "        file_path_fn=functools.partial(file_path_for_zillow_data, 'zori_rent'),\n",
    "        mean_cols=['zori_rent_value'],\n",
    "    ),\n",
    "    'temperature': Dataset(\n",
    "        load_fn=load_usa_temperature_data,\n",
    "        file_path_fn=file_path_for_temperature_data,\n",
    "        mean_cols=['mean_annual_temperature'],\n",
    "    ),\n",
    "}\n",
    "\n",
    "\n",
    "def get_sum_cols(dataset_names: List[str], year: int) -> List[str]:\n",
    "    cols = []\n",
    "    for dataset_name in dataset_names:\n",
    "        cols += ALL_DATASETS[dataset_name].sum_cols\n",
    "    return [f'{col}_{year}' for col in cols]\n",
    "\n",
    "\n",
    "def get_mean_cols(dataset_names: List[str], year: int) -> List[str]:\n",
    "    cols = []\n",
    "    for dataset_name in dataset_names:\n",
    "        cols += ALL_DATASETS[dataset_name].mean_cols\n",
    "    return [f'{col}_{year}' for col in cols]\n",
    "\n",
    "\n",
    "def load_dataset(dataset: Dataset, year: int, by=str) -> pd.DataFrame:\n",
    "    try:\n",
    "        return dataset.load_fn(dataset.file_path_fn(year, by), by=by)\n",
    "    except TypeError:\n",
    "        return dataset.load_fn(dataset.file_path_fn(year, by), by=by, year=year)\n",
    "\n",
    "\n",
    "def load_datasets(dataset_names: List[str], year: int, by: str) -> Dict[str, pd.DataFrame]:\n",
    "    datasets = {}\n",
    "    for dataset_name in dataset_names:\n",
    "        datasets[dataset_name] = load_dataset(ALL_DATASETS[dataset_name], year, by)\n",
    "    return datasets\n",
    "\n",
    "\n",
    "def create_features_df(datasets: Dict[str, pd.DataFrame]) -> pd.DataFrame:\n",
    "    coc_to_county_df = load_coc_to_county_df()\n",
    "    one_off_datasets = ['affordable_units', 'temperature']\n",
    "    feature_merge_cols = ['state_name', 'county_name'] if by == 'county' else ['state_name']\n",
    "    dfs = [v for k, v in datasets.items() if k not in one_off_datasets]\n",
    "    features_df = merge_dfs(dfs + [coc_to_county_df], feature_merge_cols)\n",
    "    if 'affordable_units' in datasets:\n",
    "        features_df = features_df.merge(datasets['affordable_units'], how='left', on=feature_merge_cols).fillna(0)\n",
    "    if 'temperature' in datasets:\n",
    "        features_df = features_df.merge(datasets['temperature'], how='inner', on=['state_name'])\n",
    "    return features_df\n",
    "\n",
    "\n",
    "def create_model_df(features_df: pd.DataFrame, year: int, by: str) -> pd.DataFrame:\n",
    "    coc_df = load_hud_coc_data('data/HUD_2007-2022-PIT-Counts-by-CoC.xlsx', year)\n",
    "    coc_data_cols = [col for col in coc_df.columns if col not in ['coc_name', 'coc_number']]\n",
    "    model_df = features_df.merge(coc_df, on=['coc_number'], how='inner')\n",
    "    model_df = model_df.rename(columns={'coc_name_x': 'coc_name'})\n",
    "    cols = ['coc_number', 'coc_name', 'state_name']\n",
    "    if by == 'county':\n",
    "        cols.append('county_name')\n",
    "    model_df = model_df.sort_values(by=cols).reset_index(drop=True)\n",
    "    return model_df, coc_data_cols\n",
    "\n",
    "\n",
    "def create_dataset(config: DataConfig) -> pd.DataFrame:\n",
    "    year = config.year\n",
    "    by = config.by\n",
    "    dataset_names = config.dataset_names\n",
    "    datasets = load_datasets(dataset_names, year, by)\n",
    "    features_df = create_features_df(datasets)\n",
    "    model_df, coc_data_cols = create_model_df(features_df, year, by)\n",
    "    \n",
    "    coc_model_df = model_df.copy(deep=True)\n",
    "    sum_cols = get_sum_cols(dataset_names, year)\n",
    "    mean_cols = get_mean_cols(dataset_names, year)\n",
    "\n",
    "    if by == 'state':\n",
    "        grouped_by_df = coc_model_df.groupby(['state_name'])\n",
    "        coc_data_df = grouped_by_df[coc_data_cols].sum().reset_index()\n",
    "        state_data_df = grouped_by_df[sum_cols + mean_cols].mean().reset_index()\n",
    "        df = merge_dfs([coc_data_df, state_data_df], cols=['state_name'])\n",
    "        df = df.sort_values(by=['state_name'])\n",
    "    \n",
    "    elif by == 'county':\n",
    "        grouped_by_df = coc_model_df.groupby(['coc_number', 'coc_name'])\n",
    "        total_mean_cols = [f'population_weighted_{col}' for col in mean_cols]\n",
    "        coc_model_df[total_mean_cols] = coc_model_df[mean_cols].multiply(coc_model_df[f'population_{year}'], axis=0)\n",
    "        coc_data_df = grouped_by_df[coc_data_cols].mean().reset_index()\n",
    "        mean_data_df = grouped_by_df[total_mean_cols].sum().reset_index()\n",
    "        sum_data_df = grouped_by_df[sum_cols].sum().reset_index()\n",
    "        df = merge_dfs([coc_data_df, mean_data_df, sum_data_df], cols=['coc_number', 'coc_name'])\n",
    "        df[mean_cols] = df[total_mean_cols].div(df[f'population_{year}'], axis=0)\n",
    "        df = df.drop(columns=total_mean_cols)\n",
    "        df = df.sort_values(by=['coc_number'])\n",
    "    \n",
    "    df.columns = [col.removesuffix(f'_{year}') for col in df.columns]\n",
    "    df['year'] = year\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "75243b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclasses.dataclass\n",
    "class DataConfig:\n",
    "    year: int\n",
    "    by: str\n",
    "    dataset_names: List[str]\n",
    "    suffix: str = ''\n",
    "\n",
    "\n",
    "common_dataset_names = [\n",
    "    'unemployment',\n",
    "    'income',\n",
    "    'all_homes',\n",
    "    'bottom_tier_homes',\n",
    "    'population',\n",
    "    'affordable_units',\n",
    "    'temperature',\n",
    "]\n",
    "\n",
    "additional_dataset_names = [\n",
    "    'gdp',\n",
    "    'social_capital',\n",
    "    'rent',\n",
    "]\n",
    "\n",
    "# years = [2016, 2017, 2018, 2019, 2020, 2021]\n",
    "# years = [2017]\n",
    "# years = list(range(2011, 2022))\n",
    "years = [\n",
    "    2021,\n",
    "    2020,\n",
    "    2019,\n",
    "    2018,\n",
    "    2017,\n",
    "    2016,\n",
    "    2015,\n",
    "#     2014,  # Here and earlier: Doesn't have as granular breakdown of homeless demographics\n",
    "#     2013,\n",
    "#     2012,\n",
    "#     2011,\n",
    "]\n",
    "\n",
    "data_configs = [\n",
    "#     *[DataConfig(year, 'state', common_dataset_names) for year in years],\n",
    "    *[DataConfig(year, 'county', common_dataset_names) for year in years],\n",
    "#     DataConfig(2021, 'county', common_dataset_names + ['social_capital'], suffix='with_social_capital'),\n",
    "#     DataConfig(2021, 'county', common_dataset_names + ['rent'], suffix='with_rent'),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7677cc99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataConfig(year=2021, by='county', dataset_names=['unemployment', 'income', 'all_homes', 'bottom_tier_homes', 'population', 'affordable_units', 'temperature'], suffix='')\n",
      "0 2997\n",
      "1 2901\n",
      "2 2851\n",
      "3 2843\n",
      "4 2806\n",
      "0 337\n",
      "1 337\n",
      "DataConfig(year=2020, by='county', dataset_names=['unemployment', 'income', 'all_homes', 'bottom_tier_homes', 'population', 'affordable_units', 'temperature'], suffix='')\n",
      "0 2967\n",
      "1 2884\n",
      "2 2834\n",
      "3 2834\n",
      "4 2797\n",
      "0 340\n",
      "1 340\n",
      "DataConfig(year=2019, by='county', dataset_names=['unemployment', 'income', 'all_homes', 'bottom_tier_homes', 'population', 'affordable_units', 'temperature'], suffix='')\n",
      "0 2967\n",
      "1 2884\n",
      "2 2834\n",
      "3 2834\n",
      "4 2797\n",
      "0 340\n",
      "1 340\n",
      "DataConfig(year=2018, by='county', dataset_names=['unemployment', 'income', 'all_homes', 'bottom_tier_homes', 'population', 'affordable_units', 'temperature'], suffix='')\n",
      "0 2967\n",
      "1 2884\n",
      "2 2834\n",
      "3 2834\n",
      "4 2797\n",
      "0 343\n",
      "1 343\n",
      "DataConfig(year=2017, by='county', dataset_names=['unemployment', 'income', 'all_homes', 'bottom_tier_homes', 'population', 'affordable_units', 'temperature'], suffix='')\n",
      "0 2967\n",
      "1 2884\n",
      "2 2834\n",
      "3 2834\n",
      "4 2797\n",
      "0 343\n",
      "1 343\n",
      "DataConfig(year=2016, by='county', dataset_names=['unemployment', 'income', 'all_homes', 'bottom_tier_homes', 'population', 'affordable_units', 'temperature'], suffix='')\n",
      "0 2967\n",
      "1 2884\n",
      "2 2834\n",
      "3 2834\n",
      "4 2797\n",
      "0 342\n",
      "1 342\n",
      "DataConfig(year=2015, by='county', dataset_names=['unemployment', 'income', 'all_homes', 'bottom_tier_homes', 'population', 'affordable_units', 'temperature'], suffix='')\n",
      "0 2967\n",
      "1 2884\n",
      "2 2834\n",
      "3 2834\n",
      "4 2797\n",
      "0 343\n",
      "1 343\n"
     ]
    }
   ],
   "source": [
    "dfs = []\n",
    "for config in data_configs:\n",
    "    print(config)\n",
    "    year = config.year\n",
    "    by = config.by\n",
    "    suffix = f'_{config.suffix}' if config.suffix else ''\n",
    "    df = create_dataset(config)\n",
    "    dfs.append(df)\n",
    "#     df.to_csv(f'data/model/model_dataset_by_{by}_{year}{suffix}.csv', index=False)    df = pd.concat(dfs)\n",
    "\n",
    "df = pd.concat(dfs)\n",
    "df.to_csv(f'data/model/model_dataset_by_{by}_{data_configs[-1].year}_to_{data_configs[0].year}{suffix}.csv', index=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf463d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
